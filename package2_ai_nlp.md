---
title: "Package 2: AI & NLP"
layout: page
---

# Package 2: AI and Natural Language Processing Sequence
**Instructor:** Utku Turk (utkuturk@umd.edu)
**Format:** Quarter System (10 weeks per course)
**Level Progression:** Mixed (Undergrad/Grad) → Advanced Graduate

---

## Package Vision

This two-course sequence bridges machine learning, NLP, and linguistic theory. Students learn to build neural models **and** use them as scientific instruments to investigate linguistic structure. Unlike typical NLP courses prioritizing engineering performance, this sequence maintains a linguistic focus: every model is interrogated for what it reveals about language. Students build models from scratch, probe their representations, and critically evaluate claims about AI capabilities. The sequence prepares students for interdisciplinary research in computational linguistics, whether in academia or industry.

---

## Course Summaries

### Course 1: Machine Learning for Language (Mixed Level, Fall Quarter)
**Prerequisites:** None. No prior programming or math experience required.

This course introduces neural networks and their applications to language, balancing technical foundations with linguistic questions. Students build neural networks from scratch—implementing backpropagation by hand, training word embeddings, constructing RNNs/LSTMs, and finally working with transformers and LLMs. But critically, every model is interrogated through three questions: (1) What can it do? (2) What can't it do? (3) What does that tell us about language? Students design probing experiments to test whether models capture syntactic dependencies, semantic relations, or merely surface heuristics. Python programming taught from scratch through hands-on labs in Google Colab.

**Key outputs:** 4 homework assignments (coding + concepts), midterm classifier proposal, final research proposal (8-10 pages) investigating a linguistic question with neural networks.

### Course 2: Advanced NLP & AI for Linguistics (Graduate Level, Winter Quarter)
**Prerequisites:** Background in neural networks (Course 1 or equivalent)

Neural networks are often treated as black boxes. In this advanced seminar, we open them up. Students learn cutting-edge interpretability methods: (1) **probing classifiers** to test for linguistic knowledge, (2) **attention analysis** to understand syntactic dependency encoding, and (3) **causal intervention** to establish causal relationships between model components and behavior. The course runs as a research workshop—students read 2024-2025 papers, lead critical discussions, implement state-of-the-art methods, and develop original research proposals. By the end, students can design rigorous interpretability experiments and contribute to debates about what neural models actually learn.

**Key outputs:** 2 paper presentations, 3 method implementations (probing, attention, causality/geometry), final research paper (10-12 pages in ACL format) with code.

---

## Learning Philosophy

This sequence embodies three core principles. **First, linguistics first, engineering second:** these courses prioritize linguistic questions over benchmark performance. **Second, critical AI literacy:** students question claims about AI capabilities—not "can LLMs do syntax?" but "what syntactic patterns do they capture?" **Third, hands-on implementation:** students implement backpropagation by hand, code transformers from scratch, and run causal interventions. Students completing both courses can build modern NLP systems, critically evaluate AI capabilities, and conduct interpretability research.

---

**Progression:** Course 1 teaches how to build and train neural models → Course 2 teaches how to probe and interpret them. Skills build cumulatively: Python/PyTorch (both courses), transformers (introduced in Course 1, analyzed in Course 2), critical evaluation (Course 1 foundations, Course 2 advanced methods).
